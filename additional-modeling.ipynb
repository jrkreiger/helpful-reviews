{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T00:19:01.275054Z",
     "start_time": "2020-02-18T00:18:57.355758Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the first packages we'll need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the text and target dataset from local copy\n",
    "data_clean = pd.read_csv('data_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, read in the text and target dataset from Google Drive\n",
    "# Connect to Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset from Drive\n",
    "data_clean = pd.read_csv(\"/content/drive/My Drive/helpful-reviews/data_clean.csv\")\n",
    "data_clean.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# Create boolean column `helpful_1`\n",
    "data_clean['helpful_1'] = np.where(data_clean['helpful'] > 0, 1, 0)\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find splitting point for 80% train, 10% validation, 10% test\n",
    "all_length = len(data_clean)\n",
    "train_len = round(0.8 * all_length)\n",
    "val_len = round(0.1 * all_length)\n",
    "\n",
    "print('Train set length:', train_len)\n",
    "print('Validation set length:', val_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-val-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data_clean['helpful_1']\n",
    "X = data_clean['text']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=val_len,\n",
    "                                                    random_state=123)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                test_size=val_len,\n",
    "                                                random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up stopwords to be removed\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_list = stopwords.words('english')\n",
    "stop_list += list(string.punctuation)\n",
    "stop_list += ['br', '.<', '..', '...', '``', \"''\", '--', 'http', 'https',\n",
    "              'com', 'www']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed packages/classes\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras import callbacks\n",
    "from keras.preprocessing import text, sequence\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences of tokens uniform in length for all reviews (~ 1 min.)\n",
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "X_train_tok = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_tok_pad = sequence.pad_sequences(X_train_tok, maxlen=1000)\n",
    "\n",
    "X_val_tok = tokenizer.texts_to_sequences(X_val)\n",
    "X_val_tok_pad = sequence.pad_sequences(X_val_tok, maxlen=1000)\n",
    "\n",
    "X_test_tok = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_tok_pad = sequence.pad_sequences(X_test_tok, maxlen=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a GRU network\n",
    "from keras import callbacks\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint('/content/drive/My Drive/helpful-reviews/gru_model_embed_50.h5',\n",
    "                                       monitor='val_acc', \n",
    "                                       save_best_only=True)\n",
    "# early_stop = callbacks.EarlyStopping(monitor='val_loss', \n",
    "#                                      min_delta=0.001, \n",
    "#                                      patience=5) \n",
    "\n",
    "embedding_size = 128\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(200000, embedding_size, \n",
    "                                    input_shape=(100,)))\n",
    "model.add(tf.keras.layers.GRU(25, return_sequences=True, input_shape=(100,)))\n",
    "model.add(tf.keras.layers.GRU(25, return_sequences=True, input_shape=(100,)))\n",
    "model.add(tf.keras.layers.GlobalMaxPool1D())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train_tok_pad, y_train, epochs=50, batch_size=2048, \n",
    "                    validation_data=(X_val_tok_pad, y_val),\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loss and accuracy over training epochs\n",
    "x = [i for i in range(1, 51)]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.plot(x, history.history['acc'], label='Train Accuracy')\n",
    "plt.plot(x, history.history['loss'], label='Train Loss')\n",
    "plt.plot(x, history.history['val_acc'], label='Val. Accuracy')\n",
    "plt.plot(x, history.history['val_loss'], label='Val. Loss')\n",
    "plt.title('Model performance over 50 training epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
